{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#write your own path\n",
    "path = \"/Users/Shadman/Desktop/Deeplearning/Project/Python_timeseries/\"\n",
    "\n",
    "filename = os.path.join(path, \"GSPC.csv\")\n",
    "names = ['Date','Open','High','Low','Close','Adj Close','Volume']\n",
    "df = pd.read_csv(filename , sep=',', header = None, names = names, index_col = False)\n",
    "\n",
    "#extract only the stock price from the CSV\n",
    "values = df.loc[0:2616]['Close'].tolist()\n",
    "\n",
    "time1 = list(range(1,701))\n",
    "time2 = list(range(500,1100))\n",
    "time3 = list(range(1000,1600))\n",
    "time4 = list(range(1500,2100))\n",
    "time5 = list(range(2000,2600))\n",
    "\n",
    "#Next Slice the dataset, similiar to cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Prep Option 1 - Normalize each data slice, not.  \n",
    "\n",
    "df_data1 = values[0:500]\n",
    "my1 = np.mean(df_data1)\n",
    "sd1 = np.std(df_data1)\n",
    "df_data1 = df_data1/sd1 - my1/sd1\n",
    "df_data_train1 = values[0:400]\n",
    "df_data_test1 = values[401:500]\n",
    "\n",
    "df_data2 = values[300:800]\n",
    "my2 = np.mean(df_data2)\n",
    "sd2 = np.std(df_data2)\n",
    "df_data2 = df_data2/sd2 - my2/sd2\n",
    "df_data_train2 = values[300:700]\n",
    "df_data_test2 = values[701:801]\n",
    "\n",
    "df_data3 = values[600:1100]\n",
    "my3 = np.mean(df_data3)\n",
    "sd3 = np.std(df_data3)\n",
    "df_data3 = df_data3/sd3 - my3/sd3\n",
    "df_data_train3 = values[600:1000]\n",
    "df_data_test3 = values[1001:1100]\n",
    "\n",
    "df_data4 = values[900:1400]\n",
    "my4 = np.mean(df_data4)\n",
    "sd4 = np.std(df_data4)\n",
    "df_data4 = df_data4/sd4 - my4/sd4\n",
    "df_data_train4 = values[900:1300]\n",
    "df_data_test4 = values[1301:1400]\n",
    "\n",
    "df_data5 = values[1200:1700]\n",
    "my5 = np.mean(df_data5)\n",
    "sd5 = np.std(df_data5)\n",
    "df_data5 = df_data5/sd5 - my5/sd5\n",
    "df_data_train5 = values[1200:1600]\n",
    "df_data_test5 = values[1601:1700]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep Option 2- Scaling Normalizer. Shows good result\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "df_data1 = values[0:700]\n",
    "df_data1 = [df_data1]\n",
    "scaler = Normalizer()\n",
    "scaler.fit(df_data1)\n",
    "df_data1 = scaler.transform(df_data1)\n",
    "df_data1 = df_data1[0]\n",
    "df_data1= list(df_data1)\n",
    "df_data_train1 = values[0:500]\n",
    "df_data_train1 = [df_data_train1]\n",
    "df_data_train1 = scaler.transform(df_data_train1)\n",
    "df_data_train1 = df_data_train1[0]\n",
    "df_data_test1 = values[501:700]\n",
    "df_data_test1 = [df_data_test1]\n",
    "df_data_test1 = scaler.transform(df_data_test1)\n",
    "df_data_test1 = df_data_test1[0]\n",
    "\n",
    "\n",
    "df_data2 = values[500:1100]\n",
    "df_data2 = [df_data2]\n",
    "scaler = Normalizer()\n",
    "scaler.fit(df_data2)\n",
    "df_data2 = scaler.transform(df_data2)\n",
    "df_data2 = df_data2[0]\n",
    "df_data_train2 = values[500:1000]\n",
    "df_data_train2 = [df_data_train2]\n",
    "df_data_train2 = scaler.transform(df_data_train2)\n",
    "df_data_train2 = df_data_train2[0]\n",
    "df_data_test2 = values[1001:1100]\n",
    "df_data_test2 = [df_data_test2]\n",
    "df_data_test2 = scaler.transform(df_data_test2)\n",
    "df_data_test2 = df_data_test2[0]\n",
    "\n",
    "\n",
    "df_data3 = values[1000:1600]\n",
    "df_data3 = [df_data3]\n",
    "scaler = Normalizer()\n",
    "scaler.fit(df_data3)\n",
    "df_data3 = scaler.transform(df_data3)\n",
    "df_data3 = df_data3[0]\n",
    "df_data_train3 = values[1000:1500]\n",
    "df_data_train3 = [df_data_train3]\n",
    "df_data_train3 = scaler.transform(df_data_train3)\n",
    "df_data_train3 = df_data_train3[0]\n",
    "df_data_test3 = values[1501:1600]\n",
    "df_data_test3 = [df_data_test3]\n",
    "df_data_test3 = scaler.transform(df_data_test3)\n",
    "df_data_test3 = df_data_test3[0]\n",
    "\n",
    "\n",
    "df_data4 = values[1500:2100]\n",
    "df_data4 = [df_data4]\n",
    "scaler = Normalizer()\n",
    "scaler.fit(df_data4)\n",
    "df_data4 = scaler.transform(df_data4)\n",
    "df_data4 = df_data4[0]\n",
    "df_data_train4 = values[1500:2000]\n",
    "df_data_train4 = [df_data_train4]\n",
    "df_data_train4 = scaler.transform(df_data_train4)\n",
    "df_data_train4 = df_data_train4[0]\n",
    "df_data_test4 = values[2001:2100]\n",
    "df_data_test4 = [df_data_test4]\n",
    "df_data_test4 = scaler.transform(df_data_test4)\n",
    "df_data_test4 = df_data_test4[0]\n",
    "\n",
    "df_data5 = values[2000:2600]\n",
    "df_data5 = [df_data5]\n",
    "scaler = Normalizer()\n",
    "scaler.fit(df_data5)\n",
    "df_data5 = scaler.transform(df_data5)\n",
    "df_data5 = df_data5[0]\n",
    "df_data_train5 = values[2000:2500]\n",
    "df_data_train5 = [df_data_train5]\n",
    "df_data_train5 = scaler.transform(df_data_train5)\n",
    "df_data_train5 = df_data_train5[0]\n",
    "df_data_test5 = values[2501:2600]\n",
    "df_data_test5 = [df_data_test5]\n",
    "df_data_test5 = scaler.transform(df_data_test5)\n",
    "df_data_test5 = df_data_test5[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep Option 3 - No data manipulation\n",
    "\n",
    "df_data1 = values[0:1000]\n",
    "df_data_train1 = values[0:900]\n",
    "df_data_test1 = values[901:1000]\n",
    "\n",
    "df_data2 = values[500:1500]\n",
    "df_data_train2 = values[500:1400]\n",
    "df_data_test2 = values[1401:1500]\n",
    "\n",
    "df_data3 = values[2000:2500]\n",
    "df_data_train3 = values[2000:2400]\n",
    "df_data_test3 = values[2401:2500]\n",
    "\n",
    "#df_data4 = values[1500:2100]\n",
    "#df_data_train4 = values[1500:2000]\n",
    "#df_data_test4 = values[2001:2100]\n",
    "\n",
    "#df_data5 = values[2000:2600]\n",
    "#df_data_train5 = values[2000:2500]\n",
    "#df_data_test5 = values[2501:2601]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_sequences(seq_size,obs):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for i in range(len(obs)-seq_size-1):\n",
    "        window = obs[i:(i+seq_size)]\n",
    "        after_window = obs[i+seq_size]\n",
    "        window =[[x] for x in window]\n",
    "        x.append(window)\n",
    "        y.append(after_window)\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "SEQ_SIZE = 10\n",
    "\n",
    "x_train1, y_train1 = to_sequences(SEQ_SIZE, df_data_train1)\n",
    "x_test1, y_test1 = to_sequences(SEQ_SIZE, df_data_test1)\n",
    "\n",
    "x_train2, y_train2 = to_sequences(SEQ_SIZE, df_data_train2)\n",
    "x_test2, y_test2 = to_sequences(SEQ_SIZE, df_data_test2)\n",
    "\n",
    "x_train3, y_train3 = to_sequences(SEQ_SIZE, df_data_train3)\n",
    "x_test3, y_test3 = to_sequences(SEQ_SIZE, df_data_test3)\n",
    "\n",
    "x_train4, y_train4 = to_sequences(SEQ_SIZE, df_data_train4)\n",
    "x_test4, y_test4 = to_sequences(SEQ_SIZE, df_data_test4)\n",
    "\n",
    "x_train5, y_train5 = to_sequences(SEQ_SIZE, df_data_train5)\n",
    "x_test5, y_test5 = to_sequences(SEQ_SIZE, df_data_test5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.callbacks import History \n",
    "history = History()\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences = True, input_dim = 1))\n",
    "    model.add(LSTM(50, return_sequences = False))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss = 'mae', optimizer = 'adam')\n",
    "    return model\n",
    "\n",
    "m = create_model()\n",
    "\n",
    "m.fit(x_train1, y_train1, validation_data=[x_test1, y_test1], verbose = 2, epochs = 50, callbacks=[history])\n",
    "pred1 = m.predict(x_test1)\n",
    "score1 = np.sqrt(metrics.mean_squared_error(pred1, y_test1))\n",
    "hist1 = history.history\n",
    "\n",
    "m.fit(x_train2, y_train2, validation_data=[x_test2, y_test2], verbose = 2, epochs = 50, callbacks=[history])\n",
    "pred2 = m.predict(x_test2)\n",
    "score2 = np.sqrt(metrics.mean_squared_error(pred2, y_test2))\n",
    "hist2 = history.history\n",
    "\n",
    "m.fit(x_train3, y_train3, validation_data=[x_test3, y_test3], verbose = 2, epochs = 50, callbacks=[history])\n",
    "pred3 = m.predict(x_test3)\n",
    "score3 = np.sqrt(metrics.mean_squared_error(pred3, y_test3))\n",
    "hist3 = history.history\n",
    "\n",
    "m.fit(x_train4, y_train4, validation_data=[x_test4, y_test4], verbose = 2, epochs = 50, callbacks=[history])\n",
    "pred4 = m.predict(x_test4)\n",
    "score4 = np.sqrt(metrics.mean_squared_error(pred4, y_test4))\n",
    "hist4 = history.history\n",
    "\n",
    "m.fit(x_train5, y_train5, validation_data=[x_test5, y_test5], verbose = 2, epochs = 50, callbacks=[history])\n",
    "pred5 = m.predict(x_test5)\n",
    "score5 = np.sqrt(metrics.mean_squared_error(pred5, y_test5))\n",
    "hist5 = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_list = [score1, score2, score3, score4, score5]\n",
    "\n",
    "RMSE_avg = sum(RMSE_list)/(len(RMSE_list))\n",
    "\n",
    "print(RMSE_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import figure, axes, pie, title, show\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20, 20))\n",
    "\n",
    "axes[0, 0].set_title('Slice01 RMSE: {}'.format(round(score1,2)))\n",
    "axes[0, 0].plot(time1, df_data1, 'ko', markersize = 1)\n",
    "axes[0, 0].plot(time1[len(df_data1)-len(pred1):len(df_data1)], pred1,  'ro', markersize = 1 )\n",
    "axes[0, 0].set_xlabel('Time')\n",
    "axes[0, 0].patch.set_facecolor('xkcd:white')\n",
    "\n",
    "axes[0, 1].set_title('Slice02 RMSE: {}'.format(round(score2,2)))\n",
    "axes[0, 1].plot(time2, df_data2, 'ko', markersize = 1)\n",
    "axes[0, 1].plot(time2[len(df_data2)-len(pred2):len(df_data2)], pred2,  'ro', markersize = 1 )\n",
    "axes[0, 1].set_xlabel('Time')\n",
    "\n",
    "axes[0, 2].set_title('Slice03 RMSE: {}'.format(round(score3,2)))\n",
    "axes[0, 2].plot(time3, df_data3, 'ko', markersize = 1)\n",
    "axes[0, 2].plot(time3[len(df_data3)-len(pred3):len(df_data3)], pred3,  'ro', markersize = 1 )\n",
    "axes[0, 2].set_xlabel('Time')\n",
    "\n",
    "axes[1, 0].set_title('Slice04 RMSE: {}'.format(round(score4,2)))\n",
    "axes[1, 0].plot(time4, df_data4, 'ko', markersize = 1)\n",
    "axes[1, 0].plot(time4[len(df_data4)-len(pred4):len(df_data4)], pred4,  'ro', markersize = 1 )\n",
    "axes[1, 0].set_xlabel('Time')\n",
    "\n",
    "axes[1, 1].set_title('Slice05 RMSE: {}'.format(round(score5,2)))\n",
    "axes[1, 1].plot(time5, df_data5, 'ko', markersize = 1)\n",
    "axes[1, 1].plot(time5[len(df_data5)-len(pred5):len(df_data5)], pred5,  'ro', markersize = 1 )\n",
    "axes[1, 1].set_xlabel('Time')\n",
    "\n",
    "\n",
    "axes[1, 2].remove()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20, 20))\n",
    "\n",
    "axes[0, 0].set_title('Slice01')\n",
    "axes[0, 0].plot(hist1['loss'])\n",
    "axes[0, 0].plot(hist1['val_loss'])\n",
    "axes[0, 0].set_ylabel('MAE loss')\n",
    "axes[0, 0].set_xlabel('epoch')\n",
    "axes[0, 0].legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "axes[0, 1].set_title('Slice02')\n",
    "axes[0, 1].plot(hist2['loss'])\n",
    "axes[0, 1].plot(hist2['val_loss'])\n",
    "axes[0, 1].set_ylabel('MAE loss')\n",
    "axes[0, 1].set_xlabel('epoch')\n",
    "axes[0, 1].legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "axes[0, 2].set_title('Slice03')\n",
    "axes[0, 2].plot(hist3['loss'])\n",
    "axes[0, 2].plot(hist3['val_loss'])\n",
    "axes[0, 2].set_ylabel('MAE loss')\n",
    "axes[0, 2].set_xlabel('epoch')\n",
    "axes[0, 2].legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "axes[1, 0].set_title('Slice04')\n",
    "axes[1, 0].plot(hist4['loss'])\n",
    "axes[1, 0].plot(hist4['val_loss'])\n",
    "axes[1, 0].set_ylabel('MAE loss')\n",
    "axes[1, 0].set_xlabel('epoch')\n",
    "axes[1, 0].legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "axes[1, 1].set_title('Slice05')\n",
    "axes[1, 1].plot(hist5['loss'])\n",
    "axes[1, 1].plot(hist5['val_loss'])\n",
    "axes[1, 1].set_ylabel('MAE loss')\n",
    "axes[1, 1].set_xlabel('epoch')\n",
    "axes[1, 1].legend(['train', 'test'], loc='upper right')\n",
    "\n",
    "\n",
    "axes[1, 2].remove()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
